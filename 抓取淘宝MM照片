#!/usr/bin/env python
#-*- coding:utf-8 -*-

"""
基于python2.7
"""

"""
1.抓取MM姓名，头像，年龄
2.资料简介和写真照片
3.写真照片按照文件夹保存到本地
"""

#不作处理直接抓取信息
import urllib
import urllib2
import re

#定义类，执行爬取工作
class Spider:
    #初始化基础页面
    def __init__(self):
        self.siteURL = "http:mm.taobao.com/json/request_top_list.htm"
    
    #定位到具体页面，并输出网址
    def getPage(self, pageIndex):
        #确定爬取的页面代码
        url = self.siteURL + "?page=" + str(pageIndex)
        #输出网址
        print (url)
        
        #构建request向url发送请求
        request = urllib2.Request(url)
        #使用urlopen获取url页面信息
        response = urllib2.urlopen(request)
        #将页面信息以‘gbk’格式编码后返回
        return response.read().decode('gbk')
    
    #定义函数，爬取内容
    def getContents(self, pageIndex):
        #调用getPage()函数，获取Index对应页面编码
        page = self.getPage(pageIndex)
        #设置正则表达式，筛选页面信息
        pattern = re.compile('<div class="list-item".*?pic-word.*?<a href="(.*?)".*?<img src="(.*?)"'
                         +'.*?<a class="lady-name.*?>(.*?)</a>.*?<strong>(.*?).*?</strong>.*?<span>(.*?)</span>', re.S)
        items = re.findall(pattern, page)
        for item in items:
            print (item[0].strip(), item[1].strip(), item[2].strip(), item[3].strip(), item[4].strip())
     
        
spider = Spider()
spider.getContents(1)
    
    
#详细代码
import re
import urllib
import urllib2
import tool
import OS

class Spider:
    #页面初始化
    def __init__(self):
        #定义基础网址
        self.siteURL = "http://www.mm.taobao.com/json/request_top_list.htm"
        #定义筛选工具
        self.tool = tool.Tool()
    
    #获取索引页面内容
    def getPage(self, pageIndex):
        #获取对应页码页面网址
        url = self.siteURL + "&page=" + str(pageIndex)
        #发送Request请求
        request = urllib2.Request(url)
        #获取相应页码代码
        response = urllib2.urlopen(request)
        #返回处理后的页码代码
        return response.read().decode('gbk')
    
    #获取索引界面所有MM的信心，list格式
    def getContent(self, pageIndex):
        #获取页码对应的页面代码
        page = self.getPage(pageIndex)
        #定义筛选条件
        pattern = re.compile('<div class="list-item".*?pic-word.*?<a href="(.*?)".*?<img src="(.*?)"'
                         +'.*?<a class="lady-name.*?>(.*?)</a>.*?<strong>(.*?).*?</strong>.*?<span>(.*?)</span>', re.S)
        #获取筛选后的内容
        items = re.finall(pattern, page)
        #定义列表一次保存筛选后的数据
        contents = []
        for item in items:
            contents.append(item[0], item[1], item[2], item[3], item[4])
        return contents
    
    #获取MM个人详情页面
    def getDetailPage(self, infoURL):
        #获取输入网页代码
        response = urllib2.urlopen(infoURL)
        return response.read().decode('gbk')
    
    #获取个人文字简介
    def getBrief(self, page):
        #定义删选规则
        pattern = re.compile('<div class="mm-aixiu-content".*?>(.*?)<!--', re.S)
        result = re.finall(pattern, page)
        return self.tool.repalce(result.group(1))
    
    #获取页面所有图片
    def getAllImg(self, page):
        pattern = re.compile('<div class="mm-aixiu-content".*?>(.*?)<!--', re.S)
        content = re.findall(pattern, page)
        #获取图片
        patternImg = re.compile('img.*?src="(.*?)"', re.S)
        #删选出图片
        images = re.findall(patternImg, content.group(1))
        return images
    
    
    #保存写真照片
    def saveImgs(self, images, name):
        num = 1
        print (u"发现", name, u"共有", len(images), u"张照片")
        for imgURL in images:
            #将图片分割位姓名和后缀
            splitPath = imgURL.split()
            #查看后缀名，确定保存的格式
            fTail = splitPath.pop()
            if len(fTail)>3:
                fTail = "jpg"
            fileName = name + "/" + str(num) + "." + fTail
            self.saveImg(imgURL, fileName)
            num += 1
            
    #保存头像
    def saveIcon(self, iconURL, name):
        spiltPath = iconURL.split('.')
        fTail = spiltPath.pop()
        filename = name + "/icon." + tTail
        self.saveImg(iconURL, filename)
        
    #保存个人简介
    def saveBrief(self, content, name):
        filename = name + "/" + name + ".txt"
        f = open(filename, "w+")
        print (u"开始保存个人信息为"， filename)
        #写入编码后的内容
        f.write(content.encode(''utf-8')
    
    #传入图片地址，文件名， 保存单张图片                           
    def saveImg(self, imgURL, filename):
        u = urllib.urlopen(imgURL)
        data = u.read()
        f = open(filename, "wb")
        print (u"保存图片为", filename)
        f.close()                       
                               
    #创建新目录
    def mkdir(self, path):
        path = path.strip()
        #判断路径是否存在
        #存在       True
        #不存在     False                       
        isExists = os.path.exists(path)
        if not isExists:
            #如果不存在则创建目录
            print (u"新建名字为", path, u'的文件夹')
            #创建目录
            os.makedirs(path)
            return True                   
        else:
            #如果目录存在则不创建，并提示目录已存在
            print (u"名为", path, "的文件夹以创建成功")
            return False
                               
    #将一页淘宝mm的信息保存起来
    def savePageInfo(self, pageIndex):
        #获取第一页淘宝mm列表
        contents = self.getContents(pageIndex)
        for item in contents:
            #item[0]是个人详情，item[1]是头像URL，item[2]是姓名，item[3]是年龄，item[4]是居住地
            print (u"发现一个模特，名字叫", item[2], u"年龄", item[3], u"，她在", item[4])
            print (u"正在保存", item[2], "的信息")
            print (u"个人地址是", item[0])
            #个人详细页面URL
            detailURL = item[0]                  
            #得到个人详情页面代码
            detailPage = self.getDetailPage(detailURL)
            #获取个人简介
            brief = self.getBrief(detailPage)
            #获取所有图片列表
            images = self.getAllImg(detailPage)    
            self.mkdir(item[2])
            #保存个人简介
            self.saveBrief(brief, item[2])
            #保存头像
            self.saveIcon(item[1], item[2])
            #保存图片
            self.saveImgs(images, item[2])
                               
    #传入起止页码， 获取图片
    def savePagesInfo(self, start, end):
        for i in range(start, end+1):                      
            print (u"正在寻找第", i, u"个地方，看看MM在不在")
            self.savePageInfo(i)
                               
#传入起止页码即可，在此传入2，10， 表示抓取第2到10页MM
spider = Spider()
spider.savePagesInfo(2, 10)
                               
                               
                               
                               
                               
                               
                               
                               
                               
